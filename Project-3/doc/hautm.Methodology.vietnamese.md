# Methodology: Ki·∫øn tr√∫c CNN v√† Gi·∫£i ph√°p cho b√†i to√°n Ph√¢n lo·∫°i ·∫¢nh

Trong ph·∫ßn n√†y, ch√∫ng ta s·∫Ω ƒëi s√¢u v√†o ph∆∞∆°ng ph√°p gi·∫£i quy·∫øt b√†i to√°n ph√¢n lo·∫°i ch√≥ m√®o (**Cats vs. Dogs Classification**). Thay v√¨ s·ª≠ d·ª•ng c√°c thu·∫≠t to√°n Machine Learning truy·ªÅn th·ªëng (nh∆∞ SVM hay KNN) v·ªën g·∫∑p h·∫°n ch·∫ø l·ªõn khi x·ª≠ l√Ω d·ªØ li·ªáu phi c·∫•u tr√∫c, d·ª± √°n n√†y √°p d·ª•ng **M·∫°ng N∆°-ron T√≠ch ch·∫≠p (Convolutional Neural Networks - CNN)** ‚Äì x∆∞∆°ng s·ªëng c·ªßa th·ªã gi√°c m√°y t√≠nh hi·ªán ƒë·∫°i.

## 1. ƒê·ªãnh nghƒ©a B√†i to√°n (Problem Formulation)

B√†i to√°n ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a l√† **Binary Classification** (Ph√¢n lo·∫°i nh·ªã ph√¢n).
Cho t·∫≠p d·ªØ li·ªáu $\mathcal{D} = \{(x_i, y_i)\}_{i=1}^{N}$, trong ƒë√≥:

* $x_i \in \mathbb{R}^{H \times W \times C}$: L√† h√¨nh ·∫£nh ƒë·∫ßu v√†o (trong d·ª± √°n n√†y, ch√∫ng ta resize v·ªÅ $128 \times 128 \times 3$).
* $y_i \in \{0, 1\}$: L√† nh√£n (label) t∆∞∆°ng ·ª©ng (0: M√®o, 1: Ch√≥).



<figure style="text-align: center;">
  <img src="/static/uploads/image/dogcat.png" 
       alt="dogcat" 
       width="999">
  <figcaption>
    <em>H√¨nh 1.1 Cats vs. Dogs Classification </em>
  </figcaption>
</figure>
M·ª•c ti√™u l√† hu·∫•n luy·ªán m·ªôt h√†m √°nh x·∫° $f(x; \theta)$ (m√¥ h√¨nh CNN v·ªõi tham s·ªë $\theta$) sao cho d·ª± ƒëo√°n $\hat{y} = f(x)$ gi·∫£m thi·ªÉu h√†m m·∫•t m√°t (**Loss Function**) tr√™n t·∫≠p d·ªØ li·ªáu.

## 2. T·∫°i sao l·∫°i ch·ªçn CNN?

M·∫°ng n∆°-ron truy·ªÅn th·ªëng (**Fully Connected Networks - MLP**) g·∫∑p hai v·∫•n ƒë·ªÅ l·ªõn khi x·ª≠ l√Ω ·∫£nh:

1.  **B√πng n·ªï tham s·ªë:** M·ªôt ·∫£nh m√†u $128 \times 128$ c√≥ $128 \times 128 \times 3 = 49,152$ ƒëi·ªÉm ·∫£nh. N·∫øu d√πng MLP, s·ªë l∆∞·ª£ng tr·ªçng s·ªë s·∫Ω qu√° l·ªõn, d·∫´n ƒë·∫øn *Overfitting*.
2.  **M·∫•t th√¥ng tin kh√¥ng gian:** Vi·ªác du·ªói ph·∫≥ng (flatten) ·∫£nh th√†nh vector 1 chi·ªÅu l√†m m·∫•t ƒëi m·ªëi quan h·ªá kh√¥ng gian gi·ªØa c√°c pixel l√¢n c·∫≠n (v√≠ d·ª•: m·∫Øt ph·∫£i n·∫±m g·∫ßn m≈©i).



**CNN gi·∫£i quy·∫øt v·∫•n ƒë·ªÅ n√†y nh·ªù 3 c∆° ch·∫ø:**

* **Local Connectivity:** C√°c n∆°-ron ch·ªâ k·∫øt n·ªëi v·ªõi m·ªôt v√πng nh·ªè c·ªßa ·∫£nh (Receptive Field).
* **Parameter Sharing:** D√πng chung m·ªôt b·ªô l·ªçc (filter) qu√©t qua to√†n b·ªô ·∫£nh, gi√∫p gi·∫£m ƒë√°ng k·ªÉ s·ªë l∆∞·ª£ng tham s·ªë.
* **Translation Invariance:** Kh·∫£ nƒÉng nh·∫≠n di·ªán ƒë·ªëi t∆∞·ª£ng d√π n√≥ n·∫±m ·ªü v·ªã tr√≠ n√†o trong ·∫£nh [1].

## 3. C·∫•u tr√∫c chi ti·∫øt c·ªßa M√¥ h√¨nh (Model Architecture)

M√¥ h√¨nh ƒë∆∞·ª£c thi·∫øt k·∫ø theo ki·∫øn tr√∫c **Sequential**, bao g·ªìm hai ph·∫ßn ch√≠nh: **Feature Extractor** (Tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng) v√† **Classifier** (B·ªô ph√¢n lo·∫°i).

### 3.1. Feature Extractor (Kh·ªëi T√≠ch ch·∫≠p)

Kh·ªëi n√†y c√≥ nhi·ªám v·ª• chuy·ªÉn ƒë·ªïi ·∫£nh g·ªëc (pixel th√¥) th√†nh c√°c ƒë·∫∑c tr∆∞ng tr·ª´u t∆∞·ª£ng (c·∫°nh, g√≥c, h√¨nh d·∫°ng).

#### a. Convolutional Layer (L·ªõp t√≠ch ch·∫≠p)
ƒê√¢y l√† l·ªõp c·ªët l√µi. M·ªôt b·ªô l·ªçc (kernel) $K$ k√≠ch th∆∞·ªõc $k \times k$ tr∆∞·ª£t qua ·∫£nh ƒë·∫ßu v√†o $I$. Gi√° tr·ªã t·∫°i v·ªã tr√≠ $(i, j)$ c·ªßa Feature Map ƒë·∫ßu ra ƒë∆∞·ª£c t√≠nh b·∫±ng ph√©p t√≠ch ch·∫≠p:

$$
(I * K)_{ij} = \sum_{m=0}^{k-1} \sum_{n=0}^{k-1} I(i+m, j+n) \cdot K(m, n) + b
$$



* **Trong d·ª± √°n:** S·ª≠ d·ª•ng c√°c kernel $3 \times 3$, s·ªë l∆∞·ª£ng filter tƒÉng d·∫ßn (v√≠ d·ª•: 32 $\rightarrow$ 64 $\rightarrow$ 128) ƒë·ªÉ h·ªçc c√°c ƒë·∫∑c tr∆∞ng t·ª´ ƒë∆°n gi·∫£n ƒë·∫øn ph·ª©c t·∫°p.

#### b. Activation Function (H√†m k√≠ch ho·∫°t)
Sau m·ªói l·ªõp t√≠ch ch·∫≠p, ta √°p d·ª•ng h√†m phi tuy·∫øn **ReLU (Rectified Linear Unit)** ƒë·ªÉ gi√∫p m√¥ h√¨nh h·ªçc ƒë∆∞·ª£c c√°c m·ªëi quan h·ªá ph·ª©c t·∫°p:

$$
f(x) = \max(0, x)
$$

ReLU gi√∫p gi·∫£i quy·∫øt v·∫•n ƒë·ªÅ *Vanishing Gradient* (tri·ªát ti√™u ƒë·∫°o h√†m) t·ªët h∆°n Sigmoid hay Tanh trong c√°c m·∫°ng s√¢u [2].

#### c. Pooling Layer (L·ªõp g·ªôp)
S·ª≠ d·ª•ng **MaxPooling** ($2 \times 2$) ƒë·ªÉ gi·∫£m k√≠ch th∆∞·ªõc kh√¥ng gian c·ªßa Feature Map ƒëi m·ªôt n·ª≠a.



* **M·ª•c ƒë√≠ch:** Gi·∫£m chi ph√≠ t√≠nh to√°n v√† gi√∫p m√¥ h√¨nh b·∫•t bi·∫øn v·ªõi c√°c d·ªãch chuy·ªÉn nh·ªè c·ªßa v·∫≠t th·ªÉ.

### 3.2. Classifier (Kh·ªëi Ph√¢n lo·∫°i)

Sau khi tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng, d·ªØ li·ªáu ƒë∆∞·ª£c ƒë∆∞a v√†o m·∫°ng Fully Connected ƒë·ªÉ ra quy·∫øt ƒë·ªãnh.

1.  **Flatten:** Du·ªói tensor 3D (v√≠ d·ª• $16 \times 16 \times 128$) th√†nh vector 1D.
2.  **Dense Layers:** C√°c l·ªõp ·∫©n k·∫øt n·ªëi ƒë·∫ßy ƒë·ªß.
3.  **Dropout:** M·ªôt k·ªπ thu·∫≠t Regularization, ng·∫´u nhi√™n "t·∫Øt" m·ªôt s·ªë n∆°-ron (v√≠ d·ª• 50%) trong qu√° tr√¨nh train ƒë·ªÉ ngƒÉn ch·∫∑n Overfitting [3].

### 3.3. Output Layer (ƒê·∫ßu ra)

D·ª±a tr√™n ƒëo·∫°n code Inference, m√¥ h√¨nh h·ªó tr·ª£ hai d·∫°ng ƒë·∫ßu ra, nh∆∞ng t·ªëi ∆∞u nh·∫•t cho b√†i to√°n nh·ªã ph√¢n l√† s·ª≠ d·ª•ng **Sigmoid**:

$$
\sigma(z) = \frac{1}{1 + e^{-z}}
$$

* Gi√° tr·ªã ƒë·∫ßu ra $\hat{y} \in [0, 1]$ bi·ªÉu th·ªã x√°c su·∫•t ·∫£nh l√† "Ch√≥".
* N·∫øu $\hat{y} < 0.5 \rightarrow$ M√®o, $\hat{y} \ge 0.5 \rightarrow$ Ch√≥.

> **L∆∞u √Ω:** Trong code deployment, ch√∫ng ta s·ª≠ d·ª•ng th√™m m·ªôt ng∆∞·ª°ng tin c·∫≠y (`CONFIDENCE_THRESHOLD = 0.7`). N·∫øu x√°c su·∫•t n·∫±m trong kho·∫£ng $(0.3, 0.7)$, h·ªá th·ªëng s·∫Ω tr·∫£ v·ªÅ **"Unknown"**. ƒê√¢y l√† c∆° ch·∫ø an to√†n ƒë·ªÉ tr√°nh vi·ªác m√¥ h√¨nh "ƒëo√°n m√≤" khi g·∫∑p d·ªØ li·ªáu nhi·ªÖu.

## 4. H√†m M·∫•t m√°t v√† T·ªëi ∆∞u h√≥a (Loss & Optimizer)

* **Loss Function:** S·ª≠ d·ª•ng **Binary Cross-Entropy**:
    $$
    L = -\frac{1}{N} \sum_{i=1}^{N} [y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i)]
    $$
    H√†m n√†y ph·∫°t n·∫∑ng c√°c d·ª± ƒëo√°n sai l·ªách l·ªõn so v·ªõi nh√£n th·ª±c t·∫ø.

* **Optimizer:** S·ª≠ d·ª•ng **Adam** (Adaptive Moment Estimation). ƒê√¢y l√† thu·∫≠t to√°n t·ªëi ∆∞u ph·ªï bi·∫øn nh·∫•t hi·ªán nay nh·ªù kh·∫£ nƒÉng t·ª± ƒëi·ªÅu ch·ªânh learning rate cho t·ª´ng tham s·ªë, gi√∫p h·ªôi t·ª• nhanh h∆°n SGD truy·ªÅn th·ªëng [4].

## 5. T·ªïng k·∫øt Ki·∫øn tr√∫c (Summary)

D∆∞·ªõi ƒë√¢y l√† b·∫£ng t√≥m t·∫Øt ki·∫øn tr√∫c m√¥ h√¨nh ƒë·ªÅ xu·∫•t cho d·ª± √°n (t∆∞∆°ng th√≠ch v·ªõi input size 128 trong code):

| Layer (Type) | Output Shape | Param # | Ch·ª©c nƒÉng |
| :--- | :--- | :--- | :--- |
| **Input** | (128, 128, 3) | 0 | Nh·∫≠n ·∫£nh RGB |
| **Conv2D + ReLU** | (126, 126, 32) | ~896 | Tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng c·∫•p th·∫•p |
| **MaxPooling2D** | (63, 63, 32) | 0 | Gi·∫£m chi·ªÅu d·ªØ li·ªáu |
| **Conv2D + ReLU** | (61, 61, 64) | ~18K | Tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng trung c·∫•p |
| **MaxPooling2D** | (30, 30, 64) | 0 | Gi·∫£m chi·ªÅu d·ªØ li·ªáu |
| **Conv2D + ReLU** | (28, 28, 128) | ~73K | Tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng cao c·∫•p |
| **MaxPooling2D** | (14, 14, 128) | 0 | Gi·∫£m chi·ªÅu d·ªØ li·ªáu |
| **Flatten** | (25088) | 0 | Chuy·ªÉn ƒë·ªïi sang vector |
| **Dense + ReLU** | (512) | ~12.8M | H·ªçc quan h·ªá phi tuy·∫øn |
| **Dropout (0.5)** | (512) | 0 | Ch·ªëng Overfitting |
| **Dense (Output)** | (1) | 513 | D·ª± ƒëo√°n x√°c su·∫•t (Sigmoid) |

---

### üìö T√†i li·ªáu tham kh·∫£o (References)
1. LeCun, Y., et al. (1998). "Gradient-based learning applied to document recognition." *Proceedings of the IEEE*.
2. Nair, V., & Hinton, G. E. (2010). "Rectified linear units improve restricted boltzmann machines." *ICML*.
3. Srivastava, N., et al. (2014). "Dropout: A simple way to prevent neural networks from overfitting." *JMLR*.
4. Kingma, D. P., & Ba, J. (2014). "Adam: A Method for Stochastic Optimization." *ICLR*.
